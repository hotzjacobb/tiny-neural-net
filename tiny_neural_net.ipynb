{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm coding a tiny neural net that I want to learn to approximate XOR. I'm going to do this without relying on a dedicated package/library for the nets' data and computation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create a representation of the number of units at each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurable variables\n",
    "number_of_hidden_layers = 1\n",
    "units_per_hidden_layer = 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out the number of units including the inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_inputs = 2 # one input for each signal, parallel input\n",
    "number_of_outputs = 1 # simulate binary output of an XOR gate\n",
    "units_count = [] # array where index 0 is the # of input units and the final index is the number of output units\n",
    "\n",
    "units_count.append(number_of_inputs)\n",
    "for i in range(number_of_hidden_layers):\n",
    "    units_count.append(units_per_hidden_layer)\n",
    "units_count.append(number_of_outputs)\n",
    "\n",
    "print(units_count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll connect the layers by initializing weights for edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "W = [] # Array of all weights in a simple, fully-connected feed-forward network\n",
    "\n",
    "for layer_idx in range(len(units_count)-1):\n",
    "    weights_from_layer_to_next = []\n",
    "    prev_layer_units = units_count[layer_idx]\n",
    "    next_layer_units = units_count[layer_idx+1]\n",
    "    for next_node_idx in range(next_layer_units):\n",
    "        weights_to_node = []\n",
    "        for prev_node_idx in range(prev_layer_units + 1): # add 1 for bias weight\n",
    "            initial_weight = random.random()\n",
    "            weights_to_node.append(initial_weight)\n",
    "        weights_from_layer_to_next.append(weights_to_node)\n",
    "    W.append(weights_from_layer_to_next)\n",
    "\n",
    "print(W)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more thing that I realized was convenient before writing the forward pass: we should create an Edge class. We will be multiplying the weights that we have already by the activation values at that point. Creating an edge class that has this activation value and the weight is helpful as it keeps our code organized, and makes us more ready for backprop, where we'll need the edges cumulative backprop. gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Edge:\n",
    "\n",
    "    def __init__(self, weight: float, input_value: int = 0):\n",
    "        self.weight = weight # this edge's weight\n",
    "        self.input_value = input_value # this edge's value before it is multiplied by the weight\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.weight)\n",
    "\n",
    "    def output_value(self) -> float :\n",
    "        return self.weight * self.input_value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now copy data from our list of weights into an array of edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "edges = [] # Array of all edges in a fully-connected feed-forward network\n",
    "\n",
    "for layer in W:\n",
    "    layer_edges = []\n",
    "    for node in layer:\n",
    "        node_edges = []\n",
    "        for edge_weight in node:\n",
    "            edge = Edge(edge_weight)\n",
    "            node_edges.append(edge)\n",
    "        layer_edges.append(node_edges)\n",
    "    edges.append(layer_edges)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're almost ready for our foward pass calculation, but first we need to decide on a non-linearity for our network. We'll use the ReLu function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x: float) -> float:\n",
    "    return x if x > 0 else 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll write our function for forward passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Callable, Union\n",
    "\n",
    "def forward_pass(input_values: tuple[int, int], tensor: List[List[List[Edge]]], activation_fn: Callable, output_fn: Union[Callable, None] = None, verbose=False) -> List[float]:\n",
    "    \n",
    "    # give edges connecting inputs to the first hidden layer their initial input value\n",
    "    for edges_to_hidden_unit in edges[0]:\n",
    "        for input_value, edge in zip(input_values + (None,), edges_to_hidden_unit):\n",
    "            is_bias_edge = input_value == None\n",
    "            if not is_bias_edge:\n",
    "                edge.input_value = input_value\n",
    "            else:\n",
    "                edge.input_value = 1\n",
    "\n",
    "    # iterate from layer to layer with forward pass now\n",
    "    for pre_activiation_edges, post_activation_edges in zip(tensor[:-1], tensor[1:]):\n",
    "        for unit_idx, hidden_unit_pre_edges in enumerate(pre_activiation_edges):\n",
    "            hidden_unit_input = 0\n",
    "            for pre_edge in hidden_unit_pre_edges:\n",
    "                hidden_unit_input += pre_edge.output_value()\n",
    "                if verbose:\n",
    "                    print('Pre-edge output value: ' + str(pre_edge.output_value()))\n",
    "            hidden_unit_output = activation_fn(hidden_unit_input)\n",
    "            # Hidden units are in terms of the hidden unit that the edge gives the output\n",
    "            # Below, to get a list of of edges for the same input hidden unit\n",
    "            # we iterate over the post edge hidden units and always use the same index\n",
    "            for hidden_unit_post_edges in post_activation_edges:\n",
    "                    hidden_unit_post_edges[unit_idx].input_value = hidden_unit_output\n",
    "            if verbose:\n",
    "                print('Hidden unit output: ' + str(hidden_unit_output))\n",
    "        for hidden_unit_post_edges in post_activation_edges: # loop over sets of nodes going to units of next layer\n",
    "            bias_idx = len(pre_activiation_edges)\n",
    "            hidden_unit_post_edges[bias_idx].input_value = 1 # set bias edge input to 1\n",
    "\n",
    "    # calculate the output\n",
    "    outputs = []\n",
    "    for edges_to_output_node in tensor[-1]:\n",
    "        output_node_value = 0\n",
    "        for edge in edges_to_output_node:\n",
    "            output_node_value += edge.output_value()\n",
    "        if output_fn is not None:\n",
    "            output_node_value = output_fn(output_node_value)\n",
    "        outputs.append(output_node_value)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x: float) -> float:\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to make some predictions without having done any training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "expected_outputs = [0, 1, 1, 0]\n",
    "\n",
    "for sample_input, expected_output in zip(sample_inputs, expected_outputs):\n",
    "    print(f'Output is {forward_pass(sample_input, edges, relu, sigmoid)[0]} for input {sample_input}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great; we can can output wrong results! Let's try writing some code for backpropagation so that we can train our network."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we realize we need a cost function. Our current network has one output node which should output the binary value, 1 or 0. This begs the question: what should we do if the network produces a value outside of this range. We definitely would need to penalize it, however, we'll take a different approach.\n",
    "\n",
    "Instead, we'll perform forward passes that before returning the final output, constrain it with a sigmoid function.\n",
    "\n",
    "For our cost function, we'll use cross-entropy loss (NLL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cross_entropy(y_hat: float, y: float) -> float: # binary cross-entropy loss (NLL)\n",
    "    if y == 1:\n",
    "        return -np.log(np.clip(y_hat, .000001, 1)) # clip to avoid taking log of 0\n",
    "    else:\n",
    "        return -np.log(np.clip(1-y_hat, .000001, 1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need the derivative of our activation function and the derivative of the pre-output + cost function to calculate updates to weights.\n",
    "\n",
    "The derivative of our activation function, ReLu, is perhaps more straightforward than the latter derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_derivative(x: float) -> float:\n",
    "    return 1 if x >= 0 else 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pre-output + cost function derivative involves the composition of the cross-entropy function which takes the sigmoid output as input. The use of the natural log in the cross-entropy function serves to cancel out the sigmoid function's derivative, which tends to 0 as the input tends to $\\pm \\inf$. In practical terms this means that learning can be very conservative when initial guesses are bad, and the natural log counters this so that learning is aggressive when very wrong. In fact, the derivative of the composition is $ \\sigma(a) - y$, where a is the original input into the sigmoid function. It's no accident that it has such an elegant and useful derivative, and happily makes for a nice one liner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_derivative(y_hat: float, y: int) -> float:\n",
    "    return y_hat - y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll write the backpropagation code."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, a quick class to help organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sample:\n",
    "\n",
    "    def __init__(self, inputs: tuple[int, int], output: int):\n",
    "        self.inputs = inputs # two ints that are either 0 or 1\n",
    "        self.output = output # The expected output of the XOR function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def backprop(tensor: List[List[List[Edge]]], samples: List[Sample], activation_grad_fn: Callable, output_cost_grad_fn: Callable, number_of_batches=1, iterations=10, alpha=1e-3) -> List[List[List[Edge]]]:\n",
    "    reversed_edges = tensor[::-1]\n",
    "    # split data into batches\n",
    "    batches = []\n",
    "    slice_size = int(len(samples) / number_of_batches) # naively leave extra samples out if it doesn't divide evenly\n",
    "    for batch_idx in range(number_of_batches):\n",
    "        start_idx = slice_size * batch_idx\n",
    "        end_idx = (slice_size * (batch_idx + 1)) - 1\n",
    "        batches.append(samples[start_idx:end_idx+1])\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        #for batch in tqdm(batches): # as of 2022: .ipynb + vscode + tqdm = silent crashes and heachaches\n",
    "        for batch in batches:\n",
    "            # weight_gradient_update_batch = np.zeros(np.shape(reversed_edges)) # array to store cumulative gradients for each edge per batch and then divide by batch size\n",
    "            # TODO: give the copy values of all 0's in a helper function\n",
    "            weight_gradient_update_batch = create_gradient_tensor(reversed_edges) # list to store cumulative gradients for each edge per batch to later compute update\n",
    "            cost_of_batch = 0\n",
    "            for sample in batch:\n",
    "\n",
    "                # run the forward pass\n",
    "                y_hat = forward_pass(sample.inputs, tensor, relu, output_fn=sigmoid)[0] # TODO: tweak this to accomodate multi-dimensional output case\n",
    "                y = sample.output\n",
    "                cost = cross_entropy(y_hat, y)\n",
    "                cost_of_batch += cost\n",
    "\n",
    "                # calculate the gradient of the output input into the cost function and weights into final node for the batch\n",
    "                # we seperate this code from other layer connections because of cost and output functions which differ from just the activation function\n",
    "                backprop_chain_value = create_gradient_tensor(reversed_edges) # only stores the current example data; for backprop\n",
    "                output_cost_gradient = output_cost_grad_fn(y_hat, y) # TODO: tweak this to accomodate multi-dimensional output case\n",
    "                for output_node_idx, output_node_edges in enumerate(reversed_edges[0]):\n",
    "                    for edge_idx, edge in enumerate(output_node_edges):\n",
    "                        weight_gradient_one_sample = output_cost_gradient * edge.input_value\n",
    "                        weight_gradient_update_batch[0][output_node_idx][edge_idx] += weight_gradient_one_sample\n",
    "                        # we need to multiply the weight b/c when we go to the prev layer we are no longer\n",
    "                        # differentiating w.r.t. to this layer's weight; it becomes a multiplier\n",
    "                        backprop_chain_value[0][output_node_idx][edge_idx] = output_cost_gradient * edge.weight\n",
    "\n",
    "                # iterate over layer connections\n",
    "                if len(reversed_edges) <= 1:\n",
    "                    raise ValueError('Tensor is not of appriate size') # there should be at least one hidden layer\n",
    "                for slice_layer_idx, layer_edges in enumerate(reversed_edges[1:]): # start from penultimate layer as we calculated output/cost gradient above\n",
    "                    layer_idx = slice_layer_idx + 1 # layer_idx corresponds to the same layer in the unsliced, reversed tensor\n",
    "                    for node_idx, node_edges in enumerate(layer_edges):\n",
    "                        post_activation_grad = 0 # the gradients of all post-activation edges summed for the node that we are looking at\n",
    "                        for post_activation_layer in backprop_chain_value[layer_idx-1]: # TODO change name to post_activation_unit\n",
    "                            for post_activation_edge_gradient in post_activation_layer[:-1]: # do not include the next layer's bias; which is unconnected\n",
    "                                post_activation_grad += post_activation_edge_gradient\n",
    "                        # TODO: Use a data structure and save hidden unit input in the forward pass\n",
    "                        hidden_unit_input = 0 # recalculate hidden unit input using values stored in \n",
    "                        for pre_act_edge_idx, pre_activation_edge in enumerate(node_edges):\n",
    "                            hidden_unit_input += pre_activation_edge.output_value()\n",
    "                        activation_function_grad = activation_grad_fn(hidden_unit_input)\n",
    "                        for pre_act_edge_idx, pre_activation_edge in enumerate(node_edges):\n",
    "                            weight_gradient_one_sample = pre_activation_edge.input_value * activation_function_grad * post_activation_grad # chain rule\n",
    "                            weight_gradient_update_batch[layer_idx][node_idx][pre_act_edge_idx] += weight_gradient_one_sample\n",
    "                            # we need to multiply the weight b/c when we go to the prev layer we are no longer\n",
    "                            # differentiating w.r.t. to this layer's weight; it becomes a multiplier\n",
    "                            backprop_chain_value[layer_idx][node_idx][pre_act_edge_idx] = activation_function_grad * post_activation_grad * pre_activation_edge.weight\n",
    "\n",
    "\n",
    "            for layer_idx, layer in enumerate(reversed_edges):\n",
    "                for node__idx, node in enumerate(layer):\n",
    "                    for edge_idx, edge in enumerate(node):\n",
    "                        edge.weight = -weight_gradient_update_batch[layer_idx][node__idx][edge_idx]/len(batch) * alpha + edge.weight # update weight\n",
    "\n",
    "            avg_cost_per_example = cost_of_batch / len(batch)\n",
    "            # print(f'The average cost per example of the batch is: {avg_cost_per_example}')\n",
    "\n",
    "    tensor_str_array = []\n",
    "    for layer in tensor:\n",
    "        layer_list = []\n",
    "        for node in layer:\n",
    "            node_list = []\n",
    "            for edge in node:\n",
    "                node_list.append(str(edge))\n",
    "            layer_list.append(node_list)\n",
    "        tensor_str_array.append(layer_list)\n",
    "    print(tensor)\n",
    "\n",
    "    return tensor\n",
    "\n",
    "# Helper function that creates a tensor as the same structure as the input tensor, but with float's representing gradient update for that edge\n",
    "def create_gradient_tensor(tensor: List[List[List[Edge]]]) -> List[List[List[float]]]:\n",
    "    gradient_tensor = []\n",
    "    for layer in tensor:\n",
    "        layer_list = []\n",
    "        for node in layer:\n",
    "            node_list = []\n",
    "            for _ in node:\n",
    "                node_list.append(0) # initialize to 0, should be modified before use in backprop\n",
    "            layer_list.append(node_list)\n",
    "        gradient_tensor.append(layer_list)\n",
    "    return  gradient_tensor\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we'll create our samples for XOR; there's not too many possibilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_low = Sample((0, 0), 0)\n",
    "left_high = Sample((1, 0), 1)\n",
    "right_high = Sample((0, 1), 1)\n",
    "both_high = Sample((1, 1), 0)\n",
    "samples_list = [both_low, left_high, right_high, both_high]\n",
    "\n",
    "shortened_samples_list_high = [left_high, right_high] # network should learn to output 1 for one hot samples; useful for debug as I suspect local min. prob is less problematic\n",
    "shortened_samples_list_low = [both_low, both_high]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backprop(edges, samples_list, activation_grad_fn=relu_derivative, output_cost_grad_fn=nll_derivative, iterations=10000, alpha=1e-2, number_of_batches=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "expected_outputs = [0, 1, 1, 0]\n",
    "\n",
    "for sample_input, expected_output in zip(sample_inputs, expected_outputs):\n",
    "    print(f'Output is {forward_pass(sample_input, edges, relu, sigmoid)[0]} for input {sample_input}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's likely that on your first try you will fall into a local minimum. As XOR has extremely low diminsionality and my optimization is decidedly unsophisticated; that is a reality. I understand why neural nets would have had their detractors before it became more known that significantly incorrect local minimums are more easily avoidable in higher dimensions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "\n",
    "There were some interesting bugs that I had to work through once I had a the code running. I think these bugs test your intuition of the fundamental concepts that underly neural nets. I had to think about \"Should a node's input determine magnitude of its update (no, although I wonder if this is stictly bad; are per-sample specialized nodes bad is an interesting question; I'm sure there's lit. on this)\", \"For backprop, are weights in the layer to the right considered? (yes)\", \"For backprop, are later layers' inputs considered? (no)\" Make sure that the forward pass is solid, and then look at backprop."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 ('cpsc532v_a1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "69f650e45cd1cb2df2d7b11844a2fbff9a8e6c638de436a37af82083b7a174d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
